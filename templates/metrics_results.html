{% extends "base.html" %}

{% block title %}Performance Metrics Analysis - AI Literature Screening Assistant{% endblock %}

{% block extra_head %}
<style>
    /* body { padding-top: 20px; padding-bottom: 40px; } */ /* Already in base.html with adjustment */
    .container { max-width: 1100px; } /* Can override base.html if needed, or use base setting */
    .metric-card { margin-bottom: 15px; padding: 15px; border: 1px solid #eee; border-radius: 5px; background-color: #f9f9f9; height: 100%;}
    .metric-value { font-size: 1.1em; font-weight: bold; }
    .matrix-table td, .matrix-table th { text-align: center; vertical-align: middle; }
    .table th, .table td { font-size: 0.9rem;}
    .match-true strong { color: green; }
    .match-false strong { color: red; }
    h2, h3, h4 { margin-bottom: 0.75em; color: #333;}
    h3 {font-size: 1.5rem;} h4 {font-size: 1.25rem;}
    .explanation { font-size: 0.85em; color: #555; margin-top: 3px;}
    .metric-group { margin-bottom: 30px; padding:15px; border: 1px solid #e7e7e7; border-radius: 5px; background-color: #fff;}
    .dl-horizontal dt { white-space: normal; } /* Allow long dt terms to wrap */
    .badge-include { background-color: #28a745; color: white; }
    .badge-exclude { background-color: #dc3545; color: white; }
    .badge-maybe { background-color: #ffc107; color: black; }
    .badge-secondary { background-color: #6c757d; color: white; }
    /* Style for main metric group cards */
    .metric-card-group { margin-bottom: 1.5rem; }
    /* Smaller card for individual overall/binary metrics */
    .metric-value-card {
        padding: 0.8rem;
        border: 1px solid #eee;
        border-radius: 4px;
        background-color: #f9f9f9;
        height: 100%; 
        margin-bottom: 1rem; 
        text-align: center;
    }
    .metric-value-card strong {
        display: block; font-size: 0.9rem; margin-bottom: 0.25rem; color: #555;
    }
    .metric-value { font-size: 1.4em; font-weight: bold; display: block; margin-bottom: 0.3rem; }
    .explanation { font-size: 0.8em; color: #666; line-height: 1.3; }
    
    /* Confusion Matrix Styling - ADDED/UPDATED */
    .matrix-table th.actual-header { background-color: #e9ecef; font-weight: 600; }
    .matrix-table th.pred-header { background-color: #e9ecef; font-weight: 600; }
    .matrix-table td {
        font-weight: 500;
        font-size: 1rem;
        text-align: center; 
        vertical-align: middle;
    }
    .cm-correct { background-color: #d4edda; } /* Light green for correct (diagonal) */
    .cm-incorrect { background-color: #f8d7da; } /* Light red for incorrect (off-diagonal) */
    
    /* Other styles */
    .table th, .table td { font-size: 0.9rem; vertical-align: middle;}
    .match-true strong { color: green; }
    .match-false strong { color: red; }
    h2, h3, h4 { margin-bottom: 0.75em; color: #333;}
    h3 {font-size: 1.5rem;} h4 {font-size: 1.25rem;}
    .dl-horizontal dt { white-space: normal; }
    .badge-include { background-color: #28a745; color: white; }
    .badge-exclude { background-color: #dc3545; color: white; }
    .badge-maybe { background-color: #ffc107; color: black; }
    .badge-secondary { background-color: #6c757d; color: white; }
    /* Style for collapsible definition header button */
    #definitionsHeader .btn-link { color: #333; text-decoration: none; }
    #definitionsHeader .btn-link:hover { color: #0056b3; }
</style>
{% endblock %}

{% block content %}
    <div class="d-flex justify-content-between align-items-center mb-4">
        <h1>Performance Metrics & Analysis</h1>
        <div>
            {% if session_id %}
            <a href="{{ url_for('screen_full_dataset', session_id=session_id) }}" class="btn btn-success btn-sm mr-2" title="Screen the full dataset from the test file using current criteria and LLM settings.">Screen Full Dataset (from Test)</a>
            {% endif %}
            {# The url_for('index') in base.html navbar already points to the main config page. #}
            {# Consider changing this to screening_actions_page if that feels more like a 'back' destination from metrics #}
            <a href="{{ url_for('screening_actions_page') }}" class="btn btn-primary btn-sm">Back to Screening Actions</a> 
        </div>
    </div>

    {% if metrics and matrix_3x3 and matrix_3x3.matrix_data and class_metrics %}
        <p>Analysis based on <strong>{{ total_samples }}</strong> samples where both AI and Human decisions (INCLUDE, MAYBE, EXCLUDE) were provided.</p>
        <hr>

        <!-- Overall Performance Section -->
        <div class="card metric-card-group">
            <div class="card-header"><h3>Overall Performance</h3></div>
            <div class="card-body">
                <div class="row">
                    <div class="col-md-4">
                        <div class="metric-value-card">
                            <strong>Overall Accuracy</strong>
                            <span class="metric-value">{{ "%.2f"|format(metrics.overall_accuracy * 100) }}%</span>
                            <p class="explanation">AI matched Human (I/M/E).</p>
                        </div>
                    </div>
                    <div class="col-md-4">
                        <div class="metric-value-card">
                             <strong>Cohen's Kappa</strong>
                             <span class="metric-value">{{ "%.2f"|format(metrics.cohens_kappa) }}</span>
                             <p class="explanation">AI-Human agreement (chance-corrected). >0.8 Exc, 0.6-0.8 Good. (Note: Result may be NaN or near 0 if there's little variation in ratings).</p>
                        </div>
                    </div>
                    <div class="col-md-4">
                         <div class="metric-value-card">
                            <strong>Discrepancy Rate</strong>
                            <span class="metric-value">{{ "%.1f"|format(metrics.discrepancy_rate) }}%</span>
                            <p class="explanation">AI-Human disagreements.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Confusion Matrix Section -->
        <div class="card metric-card-group">
             <div class="card-header"><h3>3x3 Confusion Matrix (Human vs. AI)</h3></div>
             <div class="card-body">
                <p class="explanation small mb-2">Rows: Actual (Human) decisions, Columns: AI Prediction decisions.</p>
                <table class="table table-bordered matrix-table table-sm">
                    <thead class="thead-light">
                        <tr>
                            <th scope="col" class="actual-header">&nbsp;</th>
                            <th scope="col" colspan="{{ labels_order|length }}" class="pred-header text-center">AI Prediction</th>
                        </tr>
                        <tr>
                            <th scope="col" class="actual-header">Actual (Human)</th>
                            {% for label in labels_order %}<th scope="col" class="pred-header">{{ label }}</th>{% endfor %}
                        </tr>
                    </thead>
                    <tbody>
                        {% for i in range(labels_order|length) %}
                        <tr>
                            <th scope="row" class="actual-header">{{ labels_order[i] }}</th>
                            {% for j in range(labels_order|length) %}
                            <td class="{{ 'cm-correct' if i == j else 'cm-incorrect' }}">{{ matrix_3x3.matrix_data[i][j] }}</td>
                            {% endfor %}
                        </tr>
                        {% endfor %}
                    </tbody>
                </table>
            </div>
        </div>
        
        <!-- Per-Class Metrics Section -->
        <div class="card metric-card-group">
             <div class="card-header"><h3>Per-Class Metrics (for I, M, E)</h3></div>
             <div class="card-body">
                 <div class="row">
                    {# Structure ok, maybe style metric-value-card later #}
                    {% for label, c_metrics in class_metrics.items() %}
                    <div class="col-md-4">
                        <div class="metric-value-card">
                             <h4>Class: {{ label }}</h4>
                             <strong>Precision:</strong> {{ "%.2f"|format(c_metrics.precision) }} <small class="explanation">Correctly AI-labeled '{{label}}' / Total AI-labeled '{{label}}'.</small><hr class="my-1"> 
                             <strong>Recall (Sensitivity):</strong> {{ "%.2f"|format(c_metrics.recall) }} <small class="explanation">Correctly AI-labeled '{{label}}' / Total Human-labeled '{{label}}'.</small><hr class="my-1"> 
                             <strong>F1-Score:</strong> {{ "%.2f"|format(c_metrics.f1_score) }} <small class="explanation">Balance of P & R for '{{label}}'.</small><hr class="my-1"> 
                             <strong>Specificity:</strong> {{ "%.2f"|format(c_metrics.specificity) }} <small class="explanation">Correctly AI-labeled Not '{{label}}' / Total Human Not '{{label}}'.</small>
                         </div>
                    </div>
                    {% endfor %}
                </div>
            </div>
        </div>

        <!-- Binary Task Metrics Section -->
        <div class="card metric-card-group">
            <div class="card-header"><h3>Binary Task Metrics (Focus: Identifying "INCLUDE")</h3></div>
            <div class="card-body">
                <p class="explanation small">Evaluates AI for finding INCLUDEs (MAYBE/EXCLUDE are treated as "Not Include").</p>
                 <div class="row">
                    <div class="col-lg-3 col-md-6">
                         <div class="metric-value-card">
                            <strong>Sensitivity (Recall)</strong> 
                            <span class="metric-value">{{ "%.2f"|format(metrics.sensitivity_include * 100) }}%</span>
                            <p class="explanation">AI found Human 'INCLUDE' as 'INCLUDE'.</p>
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <div class="metric-value-card">
                            <strong>Precision</strong> 
                            <span class="metric-value">{{ "%.2f"|format(metrics.precision_include * 100) }}%</span>
                            <p class="explanation">AI 'INCLUDE' was Human 'INCLUDE'.</p>
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6">
                         <div class="metric-value-card">
                            <strong>F1-Score</strong> 
                            <span class="metric-value">{{ "%.2f"|format(metrics.f1_include) }}</span>
                            <p class="explanation">Balance for INCLUDE task.</p>
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6">
                         <div class="metric-value-card">
                             <strong>Specificity</strong> 
                             <span class="metric-value">{{ "%.2f"|format(metrics.specificity_for_include_task * 100) }}%</span>
                             <p class="explanation">AI found Human 'Not INCLUDE' as 'Not INCLUDE'.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Workload & MAYBE Analysis Section -->
        <div class="card metric-card-group">
             <div class="card-header"><h3>Workload & "MAYBE" Analysis</h3></div>
             <div class="card-body">
                 {# Structure ok, maybe style metric-value-card later #}
                  <div class="row">
                     <div class="col-md-4">
                          <div class="metric-value-card">
                             <strong>Workload Reduction</strong> 
                             <span class="metric-value">{{ "%.1f"|format(metrics.workload_reduction) }}%</span>
                             <p class="explanation">% of total items AI correctly EXCLUDED (matching Human EXCLUDE).</p>
                         </div>
                     </div>
                     <div class="col-md-4">
                          <div class="metric-value-card">
                             <strong>AI MAYBE Rate</strong> 
                             <span class="metric-value">{{ "%.1f"|format(metrics.ai_maybe_rate * 100) }}%</span>
                             <p class="explanation">% of items AI labeled 'MAYBE'.</p>
                         </div>
                     </div>
                     <div class="col-md-4">
                          <div class="metric-value-card">
                              <strong>Human MAYBE Rate</strong> 
                              <span class="metric-value">{{ "%.1f"|format(metrics.human_maybe_rate * 100) }}%</span>
                              <p class="explanation">% of items Human labeled 'MAYBE'.</p>
                         </div>
                     </div>
                 </div>
                 {# ... Maybe Resolution ... #}
                 {% if maybe_resolution %}
                 <h4 class="mt-4">"MAYBE" Resolution Counts:</h4>
                  <ul class="list-group list-group-flush small">
                     <li class="list-group-item">AI=MAYBE, Human=INCLUDE: <span class="badge badge-pill badge-info">{{ maybe_resolution.ai_maybe_to_human_include }}</span> (AI uncertain on relevant item)</li>
                     <li class="list-group-item">AI=MAYBE, Human=EXCLUDE: <span class="badge badge-pill badge-info">{{ maybe_resolution.ai_maybe_to_human_exclude }}</span> (AI uncertain on irrelevant item)</li>
                     <li class="list-group-item">AI=MAYBE, Human=MAYBE: <span class="badge badge-pill badge-success">{{ maybe_resolution.ai_maybe_to_human_maybe }}</span> (Both uncertain)</li>
                     <li class="list-group-item">Human=MAYBE, AI=INCLUDE: <span class="badge badge-pill badge-primary">{{ maybe_resolution.human_maybe_to_ai_include }}</span> (AI includes item Human was unsure of)</li>
                     <li class="list-group-item">Human=MAYBE, AI=EXCLUDE: <span class="badge badge-pill badge-primary">{{ maybe_resolution.human_maybe_to_ai_exclude }}</span> (AI excludes item Human was unsure of)</li>
                 </ul>
                 {% endif %}
            </div>
        </div>
        
    {% else %}
         <div class="alert alert-warning">Metrics calculation requires valid AI and Human decisions (INCLUDE, MAYBE, or EXCLUDE) for all test items. Please ensure selections were made.</div>
    {% endif %}

    <!-- Comparison Table Section -->
    {% if comparison %}
        <div class="card metric-card-group">
             <div class="card-header"><h3>Individual Item Comparison</h3></div>
             <div class="card-body">
                 {# ... Comparison table ... #}
                 <div class="table-responsive" style="max-height: 400px; overflow-y: auto;">
                     <table class="table table-striped table-sm table-hover">
                         <thead class="thead-light" style="position: sticky; top: 0; background-color: white; z-index:1;">
                             <tr><th>#</th><th>Title</th><th>AI Decision</th><th>Your Decision</th><th>Match?</th></tr></thead>
                         <tbody>{% for item in comparison %}<tr><td>{{ loop.index }}</td><td>{{ item.title|truncate(60) if item.title else 'N/A' }}</td>
                             <td>{%if item.ai_decision=='INCLUDE'%}<span class="badge badge-include">I</span>{%elif item.ai_decision=='EXCLUDE'%}<span class="badge badge-exclude">E</span>{%elif item.ai_decision=='MAYBE'%}<span class="badge badge-maybe">M</span>{%else%}<span class="badge badge-secondary">{{item.ai_decision}}</span>{%endif%}</td>
                             <td>{%if item.human_decision=='INCLUDE'%}<span class="badge badge-include">I</span>{%elif item.human_decision=='EXCLUDE'%}<span class="badge badge-exclude">E</span>{%elif item.human_decision=='MAYBE'%}<span class="badge badge-maybe">M</span>{%else%}<span class="badge badge-secondary">{{item.human_decision}}</span>{%endif%}</td>
                             <td class="{{'match-true' if item.match else 'match-false'}}"><strong>{{'Yes' if item.match else 'No'}}</strong></td>
                         </tr>{% endfor %}</tbody></table></div>
             </div>
        </div>
    {% else %}
        <p class="mt-3 text-muted">No comparison data available (likely because no human decisions were submitted for the test sample).</p>
    {% endif %}
    
    <!-- Definitions Section (Collapsible) -->
    <div class="card metric-card-group">
        <div class="card-header" id="definitionsHeader">
            <h3 class="mb-0">
                 <button class="btn btn-link btn-block text-left collapsed" type="button" data-toggle="collapse" data-target="#definitionsCollapse" aria-expanded="false" aria-controls="definitionsCollapse">
                     Metric Definitions Guide <small>(Click to expand/collapse)</small>
                 </button>
             </h3>
         </div>
        <div id="definitionsCollapse" class="collapse" aria-labelledby="definitionsHeader">
            <div class="card-body">
                 <dl class="row small">
                     <dt class="col-sm-3">Overall Accuracy</dt><dd class="col-sm-9">Proportion of AI decisions (I/M/E) matching Human.</dd>
                     <dt class="col-sm-3">Cohen's Kappa</dt><dd class="col-sm-9">AI-Human agreement (chance-corrected). -1 (disagree) to 1 (perfect); >0.8 Exc, 0.6-0.8 Good. NaN/Low if little rating variation.</dd>
                     <dt class="col-sm-3">Discrepancy Rate</dt><dd class="col-sm-9">100% - Overall Accuracy %.</dd>
                     <dt class="col-sm-3">3x3 Confusion Matrix</dt><dd class="col-sm-9">Breakdown of Human vs AI labels (I/M/E). Diagonal = agreement. Colors: Green=Correct, Red=Incorrect.</dd>
                     <dt class="col-sm-3">Per-Class Precision</dt><dd class="col-sm-9">For a class (e.g. INCLUDE): True Positives / (True Positives + False Positives). Of those AI called Class X, how many actually were Class X?</dd>
                     <dt class="col-sm-3">Per-Class Recall</dt><dd class="col-sm-9">For a class: True Positives / (True Positives + False Negatives). Of all actual Class X, how many did AI find?</dd>
                     <dt class="col-sm-3">Per-Class F1</dt><dd class="col-sm-9">Harmonic mean of Precision & Recall for a class. Good balance measure.</dd>
                     <dt class="col-sm-3">Per-Class Specificity</dt><dd class="col-sm-9">For a class: True Negatives / (True Negatives + False Positives). Of all actual Not Class X, how many did AI correctly label as Not Class X?</dd>
                     <dt class="col-sm-3">Binary Sensitivity (INCLUDE)</dt><dd class="col-sm-9">Recall for the INCLUDE class when M/E are grouped as 'Not Include'.</dd>
                     <dt class="col-sm-3">Binary Specificity (INCLUDE task)</dt><dd class="col-sm-9">Proportion of Human 'Not INCLUDE' (M or E) AI found as 'Not INCLUDE' (AI also M or E).</dd>
                     <dt class="col-sm-3">Workload Reduction</dt><dd class="col-sm-9">% of total studies AI correctly labeled 'EXCLUDE' matching Human 'EXCLUDE'.</dd>
                     <dt class="col-sm-3">AI/Human MAYBE Rate</dt><dd class="col-sm-9">% of studies labeled 'MAYBE' by AI/Human. High AI MAYBE may mean conservative AI/unclear criteria.</dd>
                     <dt class="col-sm-3">MAYBE Resolution</dt><dd class="col-sm-9">How 'MAYBE' by one party was classified by the other; shows nature of uncertainty.</dd>
                 </dl>
             </div>
        </div>
    </div>
{% endblock %}