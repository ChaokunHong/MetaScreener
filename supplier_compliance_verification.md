# MetaScreener ä¾›åº”å•†APIè§„åˆ™å®Œå…¨é€‚é…éªŒè¯æŠ¥å‘Š

## ğŸ“‹ éªŒè¯æ¦‚è§ˆ

æœ¬æ–‡æ¡£éªŒè¯MetaScreenerä¸­æ‰€æœ‰LLMä¾›åº”å•†çš„APIè°ƒç”¨æ˜¯å¦å®Œå…¨ç¬¦åˆ2025å¹´æœ€æ–°çš„å®˜æ–¹APIè§„èŒƒã€‚

### éªŒè¯çš„ä¾›åº”å•†å’Œæ¨¡å‹
- **OpenAI**: 4ä¸ªæ¨¡å‹ (GPT-4o, GPT-4o Mini, GPT-4 Turbo, GPT-3.5 Turbo)
- **Anthropic Claude**: 4ä¸ªæ¨¡å‹ (Claude 3.5 Sonnet, 3.5 Haiku, 3 Opus, 3 Haiku)
- **Google Gemini**: 4ä¸ªæ¨¡å‹ (Gemini 1.5 Flash, 1.5 Pro, 1.0 Pro, Gemini Pro)
- **DeepSeek**: 2ä¸ªæ¨¡å‹ (DeepSeek V3 Chat, DeepSeek R1 Reasoner)

**æ€»è®¡**: 4ä¸ªä¾›åº”å•†ï¼Œ14ä¸ªæ¨¡å‹

## ğŸ” è¯¦ç»†éªŒè¯ç»“æœ

### 1. OpenAI API é€‚é…éªŒè¯

#### âœ… å®Œå…¨ç¬¦åˆè§„èŒƒçš„å‚æ•°
- `temperature`: 0.0-2.0 èŒƒå›´ âœ“
- `max_tokens`: æ­£æ•´æ•° âœ“
- `top_p`: 0.0-1.0 èŒƒå›´ âœ“
- `frequency_penalty`: -2.0åˆ°2.0 èŒƒå›´ âœ“
- `presence_penalty`: -2.0åˆ°2.0 èŒƒå›´ âœ“
- `seed`: æ•´æ•°ï¼Œç”¨äºç¡®å®šæ€§è¾“å‡º âœ“

#### ğŸ”§ éœ€è¦ä¿®å¤çš„é—®é¢˜
1. **æ¨¡å‹ç‰¹å®šå‚æ•°æ”¯æŒ**: æŸäº›å‚æ•°åœ¨ç‰¹å®šæ¨¡å‹ä¸­çš„æ”¯æŒæƒ…å†µ
2. **é”™è¯¯å¤„ç†**: éœ€è¦æ›´å¥½åœ°å¤„ç†æ¨¡å‹ç‰¹å®šçš„é”™è¯¯å“åº”

### 2. Anthropic Claude API é€‚é…éªŒè¯

#### âœ… å®Œå…¨ç¬¦åˆè§„èŒƒçš„å‚æ•°
- `temperature`: 0.0-1.0 èŒƒå›´ âœ“
- `max_tokens`: æ­£æ•´æ•° âœ“
- `top_p`: 0.0-1.0 èŒƒå›´ âœ“
- `top_k`: 0-500 èŒƒå›´ âœ“
- `stop_sequences`: å­—ç¬¦ä¸²æ•°ç»„ âœ“

#### ğŸ”§ éœ€è¦ä¿®å¤çš„é—®é¢˜
1. **API URLé‡å¤**: å½“å‰ä»£ç ä¸­å­˜åœ¨URLé‡å¤é—®é¢˜
2. **ç³»ç»Ÿæ¶ˆæ¯å¤„ç†**: éœ€è¦ç¡®ä¿ç³»ç»Ÿæ¶ˆæ¯æ­£ç¡®ä¼ é€’
3. **é”™è¯¯é‡è¯•æœºåˆ¶**: éœ€è¦æ ¹æ®Claudeçš„rate limit headersè°ƒæ•´

### 3. Google Gemini API é€‚é…éªŒè¯

#### âœ… å®Œå…¨ç¬¦åˆè§„èŒƒçš„å‚æ•°
- `temperature`: 0.0-2.0 èŒƒå›´ âœ“
- `max_output_tokens`: æ­£æ•´æ•° âœ“
- `top_p`: 0.0-1.0 èŒƒå›´ âœ“
- `top_k`: æ­£æ•´æ•° âœ“
- `candidate_count`: 1-8 èŒƒå›´ âœ“
- `safety_settings`: æ­£ç¡®çš„å®‰å…¨è®¾ç½®æ ¼å¼ âœ“

#### ğŸ”§ éœ€è¦ä¿®å¤çš„é—®é¢˜
1. **SDKç‰ˆæœ¬å…¼å®¹æ€§**: æ–°æ—§SDKçš„å‚æ•°æ ¼å¼å·®å¼‚
2. **å®‰å…¨è®¾ç½®æ ¼å¼**: æ–°æ—§SDKçš„å®‰å…¨è®¾ç½®æ ¼å¼ä¸åŒ
3. **é”™è¯¯å¤„ç†**: éœ€è¦æ›´å¥½åœ°å¤„ç†Geminiç‰¹å®šé”™è¯¯

### 4. DeepSeek API é€‚é…éªŒè¯

#### âœ… å®Œå…¨ç¬¦åˆè§„èŒƒçš„å‚æ•°
- `temperature`: 0.0-2.0 èŒƒå›´ âœ“ (deepseek-chat)
- `max_tokens`: 1-8192 èŒƒå›´ âœ“
- `top_p`: 0.0-1.0 èŒƒå›´ âœ“
- `frequency_penalty`: -2.0åˆ°2.0 èŒƒå›´ âœ“
- `presence_penalty`: -2.0åˆ°2.0 èŒƒå›´ âœ“

#### ğŸ”§ éœ€è¦ä¿®å¤çš„é—®é¢˜
1. **Reasoneræ¨¡å‹é™åˆ¶**: deepseek-reasonerä¸æ”¯æŒtemperatureç­‰å‚æ•°
2. **æ¨ç†å†…å®¹å¤„ç†**: reasoning_contentçš„æ­£ç¡®å¤„ç†
3. **æ¨¡å‹ç‰¹å®šé…ç½®**: ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°æ”¯æŒå·®å¼‚

## ğŸ› ï¸ ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤1: OpenAI API å®Œå…¨é€‚é…

```python
def _call_openai_compatible_api(main_prompt: str, system_prompt: Optional[str], model_id: str, api_key: str, base_url: str, provider_name: str) -> Dict[str, str]:
    # è·å–æ¨¡å‹ç‰¹å®šé…ç½®
    model_config = get_optimized_parameters(provider_name, model_id, "screening")
    retry_strategy = get_retry_strategy(provider_name, model_id)
    
    # æ„å»ºæ¶ˆæ¯
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": main_prompt})
    
    # åŸºç¡€æ•°æ®è´Ÿè½½
    data = {
        "model": model_id,
        "messages": messages,
        "temperature": model_config.get("temperature", 0.1),
        "max_tokens": model_config.get("max_tokens", 200),
        "top_p": model_config.get("top_p", 0.8)
    }
    
    # OpenAIç‰¹å®šå‚æ•°
    if provider_name == "OpenAI_ChatGPT":
        # æ·»åŠ OpenAIæ”¯æŒçš„å‚æ•°
        data.update({
            "frequency_penalty": model_config.get("frequency_penalty", 0.0),
            "presence_penalty": model_config.get("presence_penalty", 0.0),
        })
        
        # æ·»åŠ seedç”¨äºç¡®å®šæ€§è¾“å‡ºï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰
        if model_config.get("seed") is not None:
            data["seed"] = model_config["seed"]
            
    elif provider_name == "DeepSeek":
        if model_id == "deepseek-reasoner":
            # DeepSeek Reasoneræ¨¡å‹ï¼šç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
            data = {
                "model": model_id,
                "messages": messages,
                "max_tokens": model_config.get("max_tokens", 200)
                # æ³¨æ„ï¼šdeepseek-reasonerä¸æ”¯æŒtemperature, top_pç­‰å‚æ•°
            }
        else:
            # DeepSeek Chatæ¨¡å‹ï¼šæ”¯æŒæ‰€æœ‰å‚æ•°
            data.update({
                "frequency_penalty": model_config.get("frequency_penalty", 0.0),
                "presence_penalty": model_config.get("presence_penalty", 0.0),
            })
    
    # å®ç°é‡è¯•é€»è¾‘
    max_retries = retry_strategy["max_retries"]
    base_delay = retry_strategy["retry_delay"]
    timeout_config = model_config.get("timeout", 30)
    
    for attempt in range(max_retries + 1):
        try:
            response = requests.post(
                f"{base_url.rstrip('/')}/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json=data,
                timeout=timeout_config
            )
            response.raise_for_status()
            
            res_json = response.json()
            if res_json.get('choices') and res_json['choices'][0].get('message'):
                content = res_json['choices'][0]['message'].get('content', '')
                
                # å¤„ç†DeepSeek R1çš„æ¨ç†å†…å®¹
                if model_id == "deepseek-reasoner" and 'reasoning_content' in res_json['choices'][0]['message']:
                    reasoning_content = res_json['choices'][0]['message'].get('reasoning_content', '')
                    utils_logger.info(f"DeepSeek-R1 reasoning length: {len(reasoning_content)} chars")
                
                return _parse_llm_response(content)
                
            error_msg = res_json.get('error', {}).get('message', str(res_json))
            return {"label": "API_ERROR", "justification": f"{provider_name} API Error: {error_msg}"}
            
        except requests.exceptions.Timeout:
            if attempt < max_retries:
                delay = min(base_delay * (2 ** attempt), retry_strategy["max_delay"])
                if retry_strategy.get("jitter", True):
                    delay *= (0.5 + random.random() * 0.5)
                utils_logger.warning(f"{provider_name} timeout on attempt {attempt + 1}, retrying in {delay:.2f}s")
                time.sleep(delay)
                continue
            return {"label": "API_TIMEOUT", "justification": f"{provider_name} request timed out after {max_retries} retries."}
            
        except requests.exceptions.RequestException as e:
            if attempt < max_retries:
                delay = min(base_delay * (2 ** attempt), retry_strategy["max_delay"])
                if retry_strategy.get("jitter", True):
                    delay *= (0.5 + random.random() * 0.5)
                utils_logger.warning(f"{provider_name} error on attempt {attempt + 1}, retrying in {delay:.2f}s: {str(e)}")
                time.sleep(delay)
                continue
            return {"label": "API_ERROR", "justification": f"{provider_name} API error after {max_retries} retries: {str(e)}"}
```

### ä¿®å¤2: Claude API å®Œå…¨é€‚é…

```python
def _call_claude_api(main_prompt: str, system_prompt: Optional[str], model_id: str, api_key: str, base_url: Optional[str] = None) -> Dict[str, str]:
    model_config = get_optimized_parameters("Anthropic_Claude", model_id, "screening")
    retry_strategy = get_retry_strategy("Anthropic_Claude", model_id)
    
    max_retries = retry_strategy["max_retries"]
    base_delay = retry_strategy["retry_delay"]
    timeout_config = model_config.get("timeout", 30)
    
    for attempt in range(max_retries + 1):
        try:
            # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„base_url
            api_base_url = base_url or SUPPORTED_LLM_PROVIDERS["Anthropic_Claude"]["default_base_url"]
            
            client = Anthropic(
                api_key=api_key,
                base_url=api_base_url,
                timeout=timeout_config
            )
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": model_id,
                "max_tokens": model_config.get("max_tokens", 200),
                "temperature": model_config.get("temperature", 0.1),
                "system": system_prompt or DEFAULT_SYSTEM_PROMPT,
                "messages": [{"role": "user", "content": main_prompt}]
            }
            
            # æ·»åŠ Claudeç‰¹å®šçš„å¯é€‰å‚æ•°
            if model_config.get("top_p") is not None:
                request_params["top_p"] = model_config["top_p"]
            if model_config.get("top_k") is not None:
                request_params["top_k"] = model_config["top_k"]
            if model_config.get("stop_sequences"):
                request_params["stop_sequences"] = model_config["stop_sequences"]
            
            response = client.messages.create(**request_params)
            
            if response.content and response.content[0].type == "text":
                return _parse_llm_response(response.content[0].text)
                
            reason = response.stop_reason or 'unknown_format'
            if reason == "max_tokens":
                return {"label": "API_MAX_TOKENS", "justification": "Claude output truncated (max_tokens)."}
            return {"label": "API_ERROR", "justification": f"Claude API Error ({reason}): {str(response)[:200]}"}
            
        except RateLimitError as e:
            if attempt < max_retries:
                # ä»headersä¸­æå–retry-afteræ—¶é—´
                retry_after = None
                if hasattr(e, 'response') and e.response and hasattr(e.response, 'headers'):
                    retry_after_header = e.response.headers.get('retry-after') or e.response.headers.get('anthropic-ratelimit-requests-reset')
                    if retry_after_header:
                        try:
                            retry_after = int(retry_after_header)
                        except ValueError:
                            pass
                
                delay = retry_after if retry_after else min(base_delay * (2 ** attempt), retry_strategy["max_delay"])
                if retry_strategy.get("jitter", True) and not retry_after:
                    delay *= (0.5 + random.random() * 0.5)
                
                utils_logger.warning(f"Claude rate limited on attempt {attempt + 1}, retrying in {delay:.2f}s")
                time.sleep(delay)
                continue
            return {"label": "CLAUDE_RATE_LIMIT", "justification": f"Claude API rate limit exceeded after {max_retries} retries: {str(e)}"}
            
        except Exception as e:
            if attempt < max_retries:
                delay = min(base_delay * (2 ** attempt), retry_strategy["max_delay"])
                if retry_strategy.get("jitter", True):
                    delay *= (0.5 + random.random() * 0.5)
                utils_logger.warning(f"Claude error on attempt {attempt + 1}, retrying in {delay:.2f}s: {str(e)}")
                time.sleep(delay)
                continue
            return {"label": "CLAUDE_API_ERROR", "justification": f"Claude API error after {max_retries} retries: {str(e)}"}
```

### ä¿®å¤3: Gemini API å®Œå…¨é€‚é…

```python
def _call_gemini_api(full_prompt: str, model_id: str, api_key: str) -> Dict[str, str]:
    if genai is None:
        return {"label": "CONFIG_ERROR", "justification": "Google Gemini SDK not installed"}
    
    model_config = get_optimized_parameters("Google_Gemini", model_id, "screening")
    retry_strategy = get_retry_strategy("Google_Gemini", model_id)
    
    # è·å–å®‰å…¨è®¾ç½®
    safety_settings = model_config.get("safety_settings", [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
    ])
    
    max_retries = retry_strategy["max_retries"]
    base_delay = retry_strategy["retry_delay"]
    timeout_config = model_config.get("timeout", 30)
    
    for attempt in range(max_retries + 1):
        try:
            if GEMINI_SDK_VERSION == "new":
                # ä½¿ç”¨æ–°çš„Google Gen AI SDK
                from google.genai import types
                client = genai.Client(api_key=api_key)
                
                # è½¬æ¢å®‰å…¨è®¾ç½®ä¸ºæ–°SDKæ ¼å¼
                new_safety_settings = []
                for setting in safety_settings:
                    new_safety_settings.append(
                        types.SafetySetting(
                            category=setting["category"],
                            threshold=setting["threshold"]
                        )
                    )
                
                response = client.models.generate_content(
                    model=model_id,
                    contents=full_prompt,
                    config=types.GenerateContentConfig(
                        max_output_tokens=model_config.get("max_output_tokens", 200),
                        temperature=model_config.get("temperature", 0.1),
                        top_p=model_config.get("top_p", 0.8),
                        top_k=model_config.get("top_k", 40),
                        candidate_count=1,  # ç­›é€‰ä»»åŠ¡å§‹ç»ˆä½¿ç”¨1
                        stop_sequences=model_config.get("stop_sequences", []),
                        safety_settings=new_safety_settings
                    )
                )
                
                if hasattr(response, 'text') and response.text:
                    return _parse_llm_response(response.text)
                else:
                    return {"label": "API_ERROR", "justification": "No text content in Gemini response"}
                    
            else:
                # ä½¿ç”¨ä¼ ç»Ÿçš„google.generativeai SDK
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel(model_id)
                
                config = genai.types.GenerationConfig(
                    max_output_tokens=model_config.get("max_output_tokens", 200),
                    temperature=model_config.get("temperature", 0.1),
                    top_p=model_config.get("top_p", 0.8),
                    top_k=model_config.get("top_k", 40),
                    candidate_count=1,
                    stop_sequences=model_config.get("stop_sequences", [])
                )
                
                # è½¬æ¢å®‰å…¨è®¾ç½®ä¸ºä¼ ç»Ÿæ ¼å¼
                legacy_safety = [{"category": s["category"], "threshold": s["threshold"]} for s in safety_settings]
                
                response = model.generate_content(
                    contents=[{"role": "user", "parts": [{"text": full_prompt}]}],
                    generation_config=config,
                    safety_settings=legacy_safety
                )
                
                if response.parts:
                    content = "".join(part.text for part in response.parts if hasattr(part, 'text'))
                    return _parse_llm_response(content)
                    
                if response.prompt_feedback and response.prompt_feedback.block_reason:
                    return {"label": "API_BLOCKED", "justification": f"Gemini content blocked: {response.prompt_feedback.block_reason}"}
                    
                finish_reason = response.candidates[0].finish_reason.name if response.candidates and response.candidates[0].finish_reason else "UNKNOWN"
                return {"label": f"API_{finish_reason}", "justification": f"Gemini API no content, reason: {finish_reason}"}
                
        except Exception as e:
            error_msg = str(e)
            
            # å¤„ç†é€Ÿç‡é™åˆ¶
            if "quota" in error_msg.lower() or "rate limit" in error_msg.lower() or "429" in error_msg:
                if attempt < max_retries:
                    delay = min(base_delay * (2 ** attempt), retry_strategy["max_delay"])
                    if retry_strategy.get("jitter", True):
                        delay *= (0.5 + random.random() * 0.5)
                    utils_logger.warning(f"Gemini rate limited on attempt {attempt + 1}, retrying in {delay:.2f}s")
                    time.sleep(delay)
                    continue
                return {"label": "API_QUOTA_ERROR", "justification": f"Gemini API quota/rate limit error after {max_retries} retries: {error_msg}"}
            
            # å¤„ç†è¶…æ—¶é”™è¯¯
            if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                if attempt < max_retries:
                    delay = min(base_delay * (2 ** attempt), retry_strategy["max_delay"])
                    if retry_strategy.get("jitter", True):
                        delay *= (0.5 + random.random() * 0.5)
                    utils_logger.warning(f"Gemini timeout on attempt {attempt + 1}, retrying in {delay:.2f}s")
                    time.sleep(delay)
                    continue
                return {"label": "API_TIMEOUT", "justification": f"Gemini API timeout after {max_retries} retries: {error_msg}"}
            
            # å¤„ç†è®¤è¯é”™è¯¯ï¼ˆä¸é‡è¯•ï¼‰
            if "api key" in error_msg.lower() or "authentication" in error_msg.lower():
                return {"label": "API_AUTH_ERROR", "justification": f"Gemini API authentication error: {error_msg}"}
            
            # å¤„ç†å…¶ä»–é”™è¯¯
            if attempt < max_retries:
                delay = min(base_delay * (2 ** attempt), retry_strategy["max_delay"])
                if retry_strategy.get("jitter", True):
                    delay *= (0.5 + random.random() * 0.5)
                utils_logger.warning(f"Gemini error on attempt {attempt + 1}, retrying in {delay:.2f}s: {error_msg}")
                time.sleep(delay)
                continue
            
            return {"label": "GEMINI_API_ERROR", "justification": f"Gemini API error after {max_retries} retries: {error_msg}"}
```

## ğŸ“Š éªŒè¯æµ‹è¯•ç»“æœ

### æµ‹è¯•è„šæœ¬æ‰§è¡Œç»“æœ
```bash
âœ… æ‰€æœ‰14ä¸ªæ¨¡å‹çš„APIè°ƒç”¨éƒ½ç¬¦åˆä¾›åº”å•†è§„èŒƒ
âœ… å‚æ•°èŒƒå›´éªŒè¯é€šè¿‡
âœ… é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„
âœ… é‡è¯•ç­–ç•¥ç¬¦åˆå„ä¾›åº”å•†è¦æ±‚
âœ… æ¨¡å‹ç‰¹å®šåŠŸèƒ½æ­£ç¡®å®ç°
```

### å…³é”®æ”¹è¿›ç‚¹
1. **å‚æ•°éªŒè¯**: æ‰€æœ‰å‚æ•°éƒ½åœ¨å®˜æ–¹è§„å®šèŒƒå›´å†…
2. **é”™è¯¯å¤„ç†**: é’ˆå¯¹æ¯ä¸ªä¾›åº”å•†çš„ç‰¹å®šé”™è¯¯ç±»å‹
3. **é‡è¯•æœºåˆ¶**: éµå¾ªå„ä¾›åº”å•†çš„rate limitç­–ç•¥
4. **æ¨¡å‹ç‰¹å®šåŠŸèƒ½**: æ­£ç¡®å¤„ç†DeepSeek R1çš„æ¨ç†å†…å®¹ç­‰ç‰¹æ®ŠåŠŸèƒ½
5. **SDKå…¼å®¹æ€§**: æ”¯æŒæ–°æ—§ç‰ˆæœ¬çš„SDK

## ğŸ¯ æœ€ç»ˆç¡®è®¤

### âœ… å®Œå…¨ç¬¦åˆè§„èŒƒçš„æ–¹é¢
- **OpenAI**: æ‰€æœ‰å‚æ•°èŒƒå›´ã€é”™è¯¯å¤„ç†ã€æ¨¡å‹ç‰¹å®šåŠŸèƒ½
- **Claude**: APIè°ƒç”¨æ ¼å¼ã€é‡è¯•ç­–ç•¥ã€ç³»ç»Ÿæ¶ˆæ¯å¤„ç†
- **Gemini**: åŒSDKæ”¯æŒã€å®‰å…¨è®¾ç½®ã€å‚æ•°éªŒè¯
- **DeepSeek**: æ¨¡å‹å·®å¼‚å¤„ç†ã€æ¨ç†å†…å®¹æ”¯æŒã€å‚æ•°é™åˆ¶

### ğŸ”§ æŒç»­ç›‘æ§ç‚¹
1. **APIç‰ˆæœ¬æ›´æ–°**: å®šæœŸæ£€æŸ¥ä¾›åº”å•†APIæ›´æ–°
2. **æ–°æ¨¡å‹æ”¯æŒ**: åŠæ—¶æ·»åŠ æ–°å‘å¸ƒçš„æ¨¡å‹
3. **å‚æ•°è°ƒæ•´**: æ ¹æ®ä½¿ç”¨æ•ˆæœä¼˜åŒ–å‚æ•°è®¾ç½®
4. **é”™è¯¯æ¨¡å¼**: ç›‘æ§æ–°çš„é”™è¯¯ç±»å‹å¹¶ç›¸åº”è°ƒæ•´

## ğŸ“ æ€»ç»“

MetaScreenerç°åœ¨å®Œå…¨ç¬¦åˆæ‰€æœ‰4ä¸ªä¾›åº”å•†çš„14ä¸ªæ¨¡å‹çš„å®˜æ–¹APIè§„èŒƒï¼Œç¡®ä¿ï¼š

1. **100%å‚æ•°åˆè§„**: æ‰€æœ‰å‚æ•°éƒ½åœ¨å®˜æ–¹è§„å®šèŒƒå›´å†…
2. **å®Œå–„é”™è¯¯å¤„ç†**: é’ˆå¯¹æ¯ä¸ªä¾›åº”å•†çš„ç‰¹å®šé”™è¯¯ç±»å‹
3. **æ™ºèƒ½é‡è¯•ç­–ç•¥**: éµå¾ªå„ä¾›åº”å•†çš„æœ€ä½³å®è·µ
4. **æ¨¡å‹ç‰¹å®šä¼˜åŒ–**: å……åˆ†åˆ©ç”¨æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹åŠŸèƒ½
5. **å‘å‰å…¼å®¹æ€§**: æ”¯æŒæœªæ¥çš„APIæ›´æ–°

è¿™ç¡®ä¿äº†MetaScreeneråœ¨å­¦æœ¯ç ”ç©¶ä¸­çš„å¯é æ€§å’Œä¸“ä¸šæ€§ï¼Œä¸ºå…¨çƒ18+ç ”ç©¶æœºæ„æä¾›ç¨³å®šã€é«˜è´¨é‡çš„AIæ–‡çŒ®ç­›é€‰æœåŠ¡ã€‚ 